{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:08.613341Z","iopub.execute_input":"2025-03-03T07:03:08.613691Z","iopub.status.idle":"2025-03-03T07:03:08.621410Z","shell.execute_reply.started":"2025-03-03T07:03:08.613655Z","shell.execute_reply":"2025-03-03T07:03:08.620361Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\n/kaggle/input/equity-post-HCT-survival-predictions/data_dictionary.csv\n/kaggle/input/equity-post-HCT-survival-predictions/train.csv\n/kaggle/input/equity-post-HCT-survival-predictions/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nimport os\nfrom tqdm import tqdm\n\n# Set device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Helper function for setting random seeds for reproducibility\ndef set_random_seeds(seed_value):\n    random.seed(seed_value)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed(seed_value)\n    torch.backends.cudnn.deterministic = True\n\n# Data loading\ntrain_data = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\ntest_data = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:13.443925Z","iopub.execute_input":"2025-03-03T07:03:13.444440Z","iopub.status.idle":"2025-03-03T07:03:16.268374Z","shell.execute_reply.started":"2025-03-03T07:03:13.444397Z","shell.execute_reply":"2025-03-03T07:03:16.267326Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dropping the target and time features for feature engineering\nexclude_columns = [\"ID\", \"efs\", \"efs_time\", \"y\"]\nrace_column = train_data['race_group']\nethnicity_column = train_data['ethnicity']  # Treating ethnicity similarly as race_group\n\nfeatures = [col for col in train_data.columns if col not in exclude_columns]\n\n# Combine race_group and ethnicity into a new column for stratification\ntrain_data['race_ethnicity_group'] = train_data['race_group'].astype(str) + '_' + train_data['ethnicity'].astype(str)\ntest_data['race_ethnicity_group'] = test_data['race_group'].astype(str) + '_' + test_data['ethnicity'].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:21.274485Z","iopub.execute_input":"2025-03-03T07:03:21.274995Z","iopub.status.idle":"2025-03-03T07:03:21.295317Z","shell.execute_reply.started":"2025-03-03T07:03:21.274952Z","shell.execute_reply":"2025-03-03T07:03:21.293942Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Handling categorical variables\ncategorical_columns = []\n\nfor column in features:\n    if train_data[column].dtype == \"object\":\n        train_data[column] = train_data[column].fillna(\"NAN\")\n        test_data[column] = test_data[column].fillna(\"NAN\")\n        categorical_columns.append(column)\n    elif 'age' not in column:\n        train_data[column] = train_data[column].astype(\"str\")\n        test_data[column] = test_data[column].astype(\"str\")\n        categorical_columns.append(column)\n\n# Label encoding the categorical columns, including 'ethnicity'\ncategory_sizes = []\nembedding_dims = []\nnumerical_columns = []\ncombined_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n\nfor column in features:\n    if column in categorical_columns:\n        combined_data[column], _ = combined_data[column].factorize()\n        combined_data[column] -= combined_data[column].min()\n        combined_data[column] = combined_data[column].astype(\"int32\")\n\n        unique_values = combined_data[column].nunique()\n        category_sizes.append(unique_values)\n        embedding_dims.append(int(np.ceil(np.sqrt(unique_values))))\n    else:\n        if combined_data[column].dtype == \"float64\":\n            combined_data[column] = combined_data[column].astype(\"float32\")\n        if combined_data[column].dtype == \"int64\":\n            combined_data[column] = combined_data[column].astype(\"int32\")\n\n        mean = combined_data[column].mean()\n        std = combined_data[column].std()\n        combined_data[column] = (combined_data[column] - mean) / std\n        combined_data[column] = combined_data[column].fillna(0)\n\n        numerical_columns.append(column)\n\ntrain_data = combined_data.iloc[:len(train_data)].copy()\ntest_data = combined_data.iloc[len(train_data):].reset_index(drop=True).copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:24.360972Z","iopub.execute_input":"2025-03-03T07:03:24.361363Z","iopub.status.idle":"2025-03-03T07:03:24.970057Z","shell.execute_reply.started":"2025-03-03T07:03:24.361337Z","shell.execute_reply":"2025-03-03T07:03:24.968942Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Log transforming the target 'efs_time' values\ntrain_data[\"y\"] = train_data.efs_time.values\nmax_value = train_data.loc[train_data.efs == 1, \"efs_time\"].max()\nmin_value = train_data.loc[train_data.efs == 0, \"efs_time\"].min()\ntrain_data.loc[train_data.efs == 0, \"y\"] += max_value - min_value\ntrain_data[\"y\"] = train_data[\"y\"].rank()\ntrain_data.loc[train_data.efs == 0, \"y\"] += 2 * len(train_data)\ntrain_data[\"y\"] = train_data[\"y\"] / train_data[\"y\"].max()\ntrain_data[\"y\"] = np.log(train_data[\"y\"])\ntrain_data[\"y\"] -= train_data[\"y\"].mean()\ntrain_data[\"y\"] *= -1.0\n\n# Dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, X_categorical, X_numerical, y=None):\n        self.X_categorical = torch.tensor(X_categorical).float()\n        self.X_numerical = torch.tensor(X_numerical).float()\n        self.y = torch.tensor(y).float() if y is not None else None\n\n    def __len__(self):\n        return len(self.X_categorical)\n\n    def __getitem__(self, idx):\n        return (self.X_categorical[idx], self.X_numerical[idx], self.y[idx].unsqueeze(0)) if self.y is not None else (self.X_categorical[idx], self.X_numerical[idx])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:30.889127Z","iopub.execute_input":"2025-03-03T07:03:30.889484Z","iopub.status.idle":"2025-03-03T07:03:30.915236Z","shell.execute_reply.started":"2025-03-03T07:03:30.889455Z","shell.execute_reply":"2025-03-03T07:03:30.913989Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Model architecture\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_dim, hidden_dim, p=0.1):\n        super(ResidualBlock, self).__init__()\n\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(p),\n            nn.LeakyReLU(),\n\n            nn.Linear(hidden_dim, input_dim),\n            nn.BatchNorm1d(input_dim),\n            nn.Dropout(p)\n        )\n\n        self.activation = nn.LeakyReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.fc(x)\n        out += residual\n        return self.activation(out)\n\nclass KaplanMeierModel(nn.Module):\n    def __init__(self, category_sizes, embedding_dims, numerical_features, hidden_dim, num_blocks, p=0.3):\n        super(KaplanMeierModel, self).__init__()\n\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cat_size, emb_dim)\n            for cat_size, emb_dim in zip(category_sizes, embedding_dims)\n        ])\n\n        input_dim = sum(embedding_dims) + len(numerical_features)\n\n        self.input_fc = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(p),\n            nn.LeakyReLU()\n        )\n\n        self.residual_blocks = nn.ModuleList([\n            ResidualBlock(hidden_dim, hidden_dim, p) for _ in range(num_blocks)\n        ])\n\n        self.output_layer = nn.Linear(hidden_dim, 1)\n\n    def forward(self, X_categorical, X_numerical):\n        X_categorical = X_categorical.long()\n        embeddings = [emb(X_categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n        embeddings = [emb.flatten(start_dim=1) for emb in embeddings]\n        X = torch.cat(embeddings + [X_numerical], dim=-1)\n        X = self.input_fc(X)\n\n        for block in self.residual_blocks:\n            X = block(X)\n\n        return self.output_layer(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:34.810486Z","iopub.execute_input":"2025-03-03T07:03:34.810824Z","iopub.status.idle":"2025-03-03T07:03:34.821908Z","shell.execute_reply.started":"2025-03-03T07:03:34.810799Z","shell.execute_reply":"2025-03-03T07:03:34.820701Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training function\ndef train_model():\n    set_random_seeds(42)\n\n    oof_predictions = np.zeros(len(train_data))\n    test_predictions = np.zeros(len(test_data))\n\n    fold_count = 5\n    repeat_count = 3\n    epochs = 15\n    learning_rate = 1e-3\n    batch_size = 32\n    criterion = nn.MSELoss()\n\n    for repeat in range(repeat_count):\n        print(f\"### Repeat {repeat + 1} ###\")\n        \n        kfold = StratifiedKFold(n_splits=fold_count, shuffle=True, random_state=42 + repeat)\n        \n        # Use combined 'race_ethnicity_group' for stratification\n        for fold, (train_idx, val_idx) in enumerate(kfold.split(train_data, train_data['race_ethnicity_group'])):\n            print(f\"   Fold {fold + 1}/{fold_count}\")\n            \n            train_subset = Subset(train_dataset, train_idx)\n            val_subset = Subset(train_dataset, val_idx)\n            \n            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n            \n            model = KaplanMeierModel(category_sizes, embedding_dims, numerical_columns, 256, 10, 0.15).to(DEVICE)\n            optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.05)\n            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** epoch)\n\n            for epoch in range(epochs):\n                model.train()\n                train_loss = 0\n                for X_categorical_batch, X_numerical_batch, y_batch in train_loader:\n                    optimizer.zero_grad()\n                    output = model(X_categorical_batch.to(DEVICE), X_numerical_batch.to(DEVICE))\n                    loss = criterion(output, y_batch.to(DEVICE))\n                    loss.backward()\n                    optimizer.step()\n                    train_loss += loss.item()\n                train_loss /= len(train_loader)\n\n                model.eval()\n                val_loss = 0\n                with torch.no_grad():\n                    for X_categorical_batch, X_numerical_batch, y_batch in val_loader:\n                        output = model(X_categorical_batch.to(DEVICE), X_numerical_batch.to(DEVICE))\n                        loss = criterion(output, y_batch.to(DEVICE))\n                        val_loss += loss.item()\n                val_loss /= len(val_loader)\n\n                print(f\"      train_loss: {train_loss}  val_loss: {val_loss}  Learning Rate: {scheduler.get_lr()[0]:.6f}\")\n                scheduler.step()\n\n            model.eval()\n            val_preds = []\n            with torch.no_grad():\n                for X_categorical_batch, X_numerical_batch, _ in val_loader:\n                    output = model(X_categorical_batch.to(DEVICE), X_numerical_batch.to(DEVICE))\n                    val_preds.append(output.cpu())\n\n            oof_predictions[val_idx] = np.concatenate(val_preds).squeeze()\n\n            test_preds = []\n            with torch.no_grad():\n                for X_categorical_batch, X_numerical_batch in test_loader:\n                    output = model(X_categorical_batch.to(DEVICE), X_numerical_batch.to(DEVICE))\n                    test_preds.append(output.cpu())\n\n            test_predictions += np.concatenate(test_preds).squeeze()\n\n    oof_predictions /= repeat_count\n    test_predictions /= (fold_count * repeat_count)\n\n    return oof_predictions, test_predictions\n\n# Prepare dataset\ntrain_dataset = CustomDataset(train_data[categorical_columns].values, train_data[numerical_columns].values, train_data['y'].values)\ntest_dataset = CustomDataset(test_data[categorical_columns].values, test_data[numerical_columns].values)\n\n# Create DataLoader for test dataset\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:39.969634Z","iopub.execute_input":"2025-03-03T07:03:39.969987Z","iopub.status.idle":"2025-03-03T07:03:39.991966Z","shell.execute_reply.started":"2025-03-03T07:03:39.969957Z","shell.execute_reply":"2025-03-03T07:03:39.990975Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Train and get predictions\noof, test_preds = train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T07:03:46.577037Z","iopub.execute_input":"2025-03-03T07:03:46.577444Z","iopub.status.idle":"2025-03-03T08:35:48.123456Z","shell.execute_reply.started":"2025-03-03T07:03:46.577413Z","shell.execute_reply":"2025-03-03T08:35:48.122199Z"}},"outputs":[{"name":"stdout","text":"### Repeat 1 ###\n   Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:382: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  _warn_get_lr_called_within_step(self)\n","output_type":"stream"},{"name":"stdout","text":"      train_loss: 2.389321134487788  val_loss: 2.0829470223850675  Learning Rate: 0.001000\n      train_loss: 2.0335757730735673  val_loss: 1.9947398861249288  Learning Rate: 0.000900\n      train_loss: 1.9610512465238572  val_loss: 2.0959809008571835  Learning Rate: 0.000810\n      train_loss: 1.8963669230540594  val_loss: 1.9319616622394986  Learning Rate: 0.000729\n      train_loss: 1.860611065145996  val_loss: 1.8789061619175804  Learning Rate: 0.000656\n      train_loss: 1.8254302915599612  val_loss: 1.8938949743906657  Learning Rate: 0.000590\n      train_loss: 1.799934014512433  val_loss: 1.930923150976499  Learning Rate: 0.000531\n      train_loss: 1.7751924260622924  val_loss: 1.867977135711246  Learning Rate: 0.000478\n      train_loss: 1.734413973407613  val_loss: 1.86563896867964  Learning Rate: 0.000430\n      train_loss: 1.7081831917994552  val_loss: 1.9029126425584157  Learning Rate: 0.000387\n      train_loss: 1.6633104326824346  val_loss: 1.8840005358060201  Learning Rate: 0.000349\n      train_loss: 1.6241664781338638  val_loss: 1.9164266129334768  Learning Rate: 0.000314\n      train_loss: 1.59667477880915  val_loss: 1.8933188590738508  Learning Rate: 0.000282\n      train_loss: 1.556140040192339  val_loss: 1.9724521537621815  Learning Rate: 0.000254\n      train_loss: 1.5251102160662413  val_loss: 1.9344669252634048  Learning Rate: 0.000229\n   Fold 2/5\n      train_loss: 2.4646672632131312  val_loss: 1.9937835137049358  Learning Rate: 0.001000\n      train_loss: 2.058100903696484  val_loss: 1.87864002916548  Learning Rate: 0.000900\n      train_loss: 1.9644185814592574  val_loss: 1.9785380525721443  Learning Rate: 0.000810\n      train_loss: 1.917133666243818  val_loss: 1.814615421824985  Learning Rate: 0.000729\n      train_loss: 1.879143903983964  val_loss: 1.8381392962402767  Learning Rate: 0.000656\n      train_loss: 1.855862574196524  val_loss: 1.7971015026172001  Learning Rate: 0.000590\n      train_loss: 1.825746503058407  val_loss: 1.8417450057135687  Learning Rate: 0.000531\n      train_loss: 1.7851717383497292  val_loss: 1.9175911453035144  Learning Rate: 0.000478\n      train_loss: 1.7516163293686178  val_loss: 1.8498795691463683  Learning Rate: 0.000430\n      train_loss: 1.7177090622484683  val_loss: 1.8328379415803486  Learning Rate: 0.000387\n      train_loss: 1.687027708441019  val_loss: 1.8404179179006153  Learning Rate: 0.000349\n      train_loss: 1.6196739010512828  val_loss: 1.8358724554379782  Learning Rate: 0.000314\n      train_loss: 1.5902884501549932  val_loss: 1.9044890738195843  Learning Rate: 0.000282\n      train_loss: 1.5558110595577292  val_loss: 1.8960317512353262  Learning Rate: 0.000254\n      train_loss: 1.5226641768382656  val_loss: 1.8897878915071487  Learning Rate: 0.000229\n   Fold 3/5\n      train_loss: 2.3726597708960373  val_loss: 2.211989431248771  Learning Rate: 0.001000\n      train_loss: 1.9974488466978073  val_loss: 2.109659649266137  Learning Rate: 0.000900\n      train_loss: 1.9255456275410123  val_loss: 2.0832470062706205  Learning Rate: 0.000810\n      train_loss: 1.8701112790240182  val_loss: 2.1105583250522613  Learning Rate: 0.000729\n      train_loss: 1.8385207970937094  val_loss: 2.00443653066953  Learning Rate: 0.000656\n      train_loss: 1.8116324160661963  val_loss: 2.021743655204773  Learning Rate: 0.000590\n      train_loss: 1.7795311882264084  val_loss: 2.0066966629690595  Learning Rate: 0.000531\n      train_loss: 1.7443760029143758  val_loss: 2.0024924404091307  Learning Rate: 0.000478\n      train_loss: 1.7098907152811686  val_loss: 1.9988740066687265  Learning Rate: 0.000430\n      train_loss: 1.6719258348147075  val_loss: 2.041468482216199  Learning Rate: 0.000387\n      train_loss: 1.649601303289334  val_loss: 2.005414723687702  Learning Rate: 0.000349\n      train_loss: 1.614233898702595  val_loss: 1.996796946724256  Learning Rate: 0.000314\n      train_loss: 1.5614707759684987  val_loss: 2.0361038929886286  Learning Rate: 0.000282\n      train_loss: 1.5246268507921032  val_loss: 2.0266244613462026  Learning Rate: 0.000254\n      train_loss: 1.4892306129965518  val_loss: 2.067398136854172  Learning Rate: 0.000229\n   Fold 4/5\n      train_loss: 2.4292323168781067  val_loss: 1.9382480694188011  Learning Rate: 0.001000\n      train_loss: 2.05026219247116  val_loss: 1.8936890582243602  Learning Rate: 0.000900\n      train_loss: 1.9755161767204603  val_loss: 1.7922446413172617  Learning Rate: 0.000810\n      train_loss: 1.9137051554189788  val_loss: 1.7996883951955371  Learning Rate: 0.000729\n      train_loss: 1.8873096456958187  val_loss: 1.8656799491908815  Learning Rate: 0.000656\n      train_loss: 1.8423351507220003  val_loss: 1.8123654541042116  Learning Rate: 0.000590\n      train_loss: 1.8098002472685444  val_loss: 1.808500337600708  Learning Rate: 0.000531\n      train_loss: 1.780348945243491  val_loss: 1.923583338989152  Learning Rate: 0.000478\n      train_loss: 1.7339237671759393  val_loss: 1.7837898545795017  Learning Rate: 0.000430\n      train_loss: 1.6974095049831601  val_loss: 1.8361541526185141  Learning Rate: 0.000387\n      train_loss: 1.6517029042045275  val_loss: 1.8358155849907134  Learning Rate: 0.000349\n      train_loss: 1.6017753433850077  val_loss: 1.9224967701567544  Learning Rate: 0.000314\n      train_loss: 1.5687356014218596  val_loss: 1.848310966624154  Learning Rate: 0.000282\n      train_loss: 1.5179312469230757  val_loss: 1.8825396928522322  Learning Rate: 0.000254\n      train_loss: 1.4750102537373702  val_loss: 1.932817774017652  Learning Rate: 0.000229\n   Fold 5/5\n      train_loss: 2.3783460607131324  val_loss: 2.022778626945284  Learning Rate: 0.001000\n      train_loss: 2.0257539371649425  val_loss: 2.1005194551414914  Learning Rate: 0.000900\n      train_loss: 1.9547176969548066  val_loss: 1.9638744844330682  Learning Rate: 0.000810\n      train_loss: 1.8967280997998184  val_loss: 1.9128341727786593  Learning Rate: 0.000729\n      train_loss: 1.8707070156931878  val_loss: 1.8952374590767755  Learning Rate: 0.000656\n      train_loss: 1.8302611677183045  val_loss: 1.9175203416082593  Learning Rate: 0.000590\n      train_loss: 1.8059543785949548  val_loss: 1.936035497321023  Learning Rate: 0.000531\n      train_loss: 1.7727884935008156  val_loss: 1.8979148301813338  Learning Rate: 0.000478\n      train_loss: 1.7221634066767162  val_loss: 1.9054431524541644  Learning Rate: 0.000430\n      train_loss: 1.691737641642491  val_loss: 1.9040984842512343  Learning Rate: 0.000387\n      train_loss: 1.6645843534833855  val_loss: 1.990526760949029  Learning Rate: 0.000349\n      train_loss: 1.6293413180443976  val_loss: 1.920214656326506  Learning Rate: 0.000314\n      train_loss: 1.5794078460998004  val_loss: 1.970203482111295  Learning Rate: 0.000282\n      train_loss: 1.5501521825790405  val_loss: 1.9438371933168834  Learning Rate: 0.000254\n      train_loss: 1.499374028791984  val_loss: 1.9614663594298893  Learning Rate: 0.000229\n### Repeat 2 ###\n   Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:382: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  _warn_get_lr_called_within_step(self)\n","output_type":"stream"},{"name":"stdout","text":"      train_loss: 2.354656480749448  val_loss: 2.018621286418703  Learning Rate: 0.001000\n      train_loss: 2.0252319660451676  val_loss: 1.9385137730174595  Learning Rate: 0.000900\n      train_loss: 1.9550073835584851  val_loss: 1.9063912673128975  Learning Rate: 0.000810\n      train_loss: 1.9088856102691756  val_loss: 1.8740088433027267  Learning Rate: 0.000729\n      train_loss: 1.8615449654559295  val_loss: 1.9044641339116626  Learning Rate: 0.000656\n      train_loss: 1.8372328345146445  val_loss: 1.9057079838381874  Learning Rate: 0.000590\n      train_loss: 1.8084106830259163  val_loss: 1.8771160705222023  Learning Rate: 0.000531\n      train_loss: 1.7774008482694625  val_loss: 1.895076286792755  Learning Rate: 0.000478\n      train_loss: 1.735729813741313  val_loss: 1.905738667315907  Learning Rate: 0.000430\n      train_loss: 1.7251389857795503  val_loss: 1.912169842918714  Learning Rate: 0.000387\n      train_loss: 1.6744410461021795  val_loss: 1.955433529946539  Learning Rate: 0.000349\n      train_loss: 1.6234197575185034  val_loss: 1.9050194200542239  Learning Rate: 0.000314\n      train_loss: 1.591028288172351  val_loss: 1.9118846343623268  Learning Rate: 0.000282\n      train_loss: 1.5443680956959724  val_loss: 1.950216316183408  Learning Rate: 0.000254\n      train_loss: 1.5238648372391859  val_loss: 1.9849231160349317  Learning Rate: 0.000229\n   Fold 2/5\n      train_loss: 2.404044520441029  val_loss: 1.9760551386409335  Learning Rate: 0.001000\n      train_loss: 2.0407368672390778  val_loss: 1.9701461702585221  Learning Rate: 0.000900\n      train_loss: 1.9523459379871686  val_loss: 1.9284800228145387  Learning Rate: 0.000810\n      train_loss: 1.9093389150997002  val_loss: 1.8948120560910966  Learning Rate: 0.000729\n      train_loss: 1.8819535804291567  val_loss: 1.8711409042278926  Learning Rate: 0.000656\n      train_loss: 1.8444076565404732  val_loss: 1.8949877980682586  Learning Rate: 0.000590\n      train_loss: 1.8125582178433737  val_loss: 1.8775152822335561  Learning Rate: 0.000531\n      train_loss: 1.7873328762749832  val_loss: 1.8702778706947962  Learning Rate: 0.000478\n      train_loss: 1.74005810626679  val_loss: 1.8686285545428594  Learning Rate: 0.000430\n      train_loss: 1.7062988390525182  val_loss: 1.8543190475967195  Learning Rate: 0.000387\n      train_loss: 1.6699456914431519  val_loss: 1.893292870786455  Learning Rate: 0.000349\n      train_loss: 1.6342372111148304  val_loss: 1.888583853840828  Learning Rate: 0.000314\n      train_loss: 1.5831548648575942  val_loss: 1.9131460189819336  Learning Rate: 0.000282\n      train_loss: 1.5527662803729376  val_loss: 1.9344935446977616  Learning Rate: 0.000254\n      train_loss: 1.5011523702906238  val_loss: 1.9359285285075505  Learning Rate: 0.000229\n   Fold 3/5\n      train_loss: 2.3753607907229  val_loss: 2.026235098308987  Learning Rate: 0.001000\n      train_loss: 2.0393194425437184  val_loss: 2.015227109193802  Learning Rate: 0.000900\n      train_loss: 1.9444439541134568  val_loss: 1.9693515290816626  Learning Rate: 0.000810\n      train_loss: 1.9018764958613448  val_loss: 1.924901732802391  Learning Rate: 0.000729\n      train_loss: 1.8669221924410926  val_loss: 1.988314367002911  Learning Rate: 0.000656\n      train_loss: 1.8395743226011594  val_loss: 1.9244491895039877  Learning Rate: 0.000590\n      train_loss: 1.808410485751099  val_loss: 1.9004724234342576  Learning Rate: 0.000531\n      train_loss: 1.7664028849866655  val_loss: 1.889068439271715  Learning Rate: 0.000478\n      train_loss: 1.7394256158007515  val_loss: 1.8931870821449492  Learning Rate: 0.000430\n      train_loss: 1.7108219930695163  val_loss: 1.8966338727209302  Learning Rate: 0.000387\n      train_loss: 1.6606510631740092  val_loss: 1.9231753259897233  Learning Rate: 0.000349\n      train_loss: 1.6237113054427836  val_loss: 1.9141845815711551  Learning Rate: 0.000314\n      train_loss: 1.5797735658784708  val_loss: 1.9398188650608064  Learning Rate: 0.000282\n      train_loss: 1.5304743932353126  val_loss: 1.9203484101427926  Learning Rate: 0.000254\n      train_loss: 1.4930906030038993  val_loss: 1.951509941286511  Learning Rate: 0.000229\n   Fold 4/5\n      train_loss: 2.4453530847198435  val_loss: 2.0772145476606156  Learning Rate: 0.001000\n      train_loss: 2.0544448089268474  val_loss: 2.0922035071584912  Learning Rate: 0.000900\n      train_loss: 1.946860041966041  val_loss: 1.968889206316736  Learning Rate: 0.000810\n      train_loss: 1.8992183119886452  val_loss: 1.9354784296618568  Learning Rate: 0.000729\n      train_loss: 1.8523763665722477  val_loss: 1.9335191580984328  Learning Rate: 0.000656\n      train_loss: 1.8245248545375135  val_loss: 1.974092302719752  Learning Rate: 0.000590\n      train_loss: 1.7945619995395343  val_loss: 1.912714457180765  Learning Rate: 0.000531\n      train_loss: 1.7591839686863953  val_loss: 1.9303507169087728  Learning Rate: 0.000478\n      train_loss: 1.7278788584801885  val_loss: 1.952444432179133  Learning Rate: 0.000430\n      train_loss: 1.6819837470021513  val_loss: 1.9393653445773655  Learning Rate: 0.000387\n      train_loss: 1.635501448644532  val_loss: 1.9642076820135117  Learning Rate: 0.000349\n      train_loss: 1.6110103657676114  val_loss: 1.9763030062119167  Learning Rate: 0.000314\n      train_loss: 1.5516373004350397  val_loss: 1.9829807705349392  Learning Rate: 0.000282\n      train_loss: 1.512318177604013  val_loss: 2.0083507988188  Learning Rate: 0.000254\n      train_loss: 1.4917470124032763  val_loss: 2.0140297992361917  Learning Rate: 0.000229\n   Fold 5/5\n      train_loss: 2.4306906157069736  val_loss: 2.0598714192708334  Learning Rate: 0.001000\n      train_loss: 2.0590756512350508  val_loss: 1.9483112116654715  Learning Rate: 0.000900\n      train_loss: 1.9751893963250848  val_loss: 1.9096017211675644  Learning Rate: 0.000810\n      train_loss: 1.9245655803216828  val_loss: 1.9692399157418146  Learning Rate: 0.000729\n      train_loss: 1.8759949592252572  val_loss: 1.914298466179106  Learning Rate: 0.000656\n      train_loss: 1.851596506188313  val_loss: 1.8782958616813024  Learning Rate: 0.000590\n      train_loss: 1.8170201893481943  val_loss: 1.886455503768391  Learning Rate: 0.000531\n      train_loss: 1.7833865470356411  val_loss: 1.8672381662660176  Learning Rate: 0.000478\n      train_loss: 1.7488351920412646  val_loss: 1.866237368186315  Learning Rate: 0.000430\n      train_loss: 1.709320323086447  val_loss: 1.889318393667539  Learning Rate: 0.000387\n      train_loss: 1.6615578991671403  val_loss: 1.8976227929194769  Learning Rate: 0.000349\n      train_loss: 1.6097733048101266  val_loss: 1.9406000041299396  Learning Rate: 0.000314\n      train_loss: 1.5711296202407943  val_loss: 1.8928674654828177  Learning Rate: 0.000282\n      train_loss: 1.5348456772665182  val_loss: 1.9130921181705263  Learning Rate: 0.000254\n      train_loss: 1.5071765580938923  val_loss: 1.9436707519822651  Learning Rate: 0.000229\n### Repeat 3 ###\n   Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:382: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  _warn_get_lr_called_within_step(self)\n","output_type":"stream"},{"name":"stdout","text":"      train_loss: 2.461884361671077  val_loss: 2.0870539949999913  Learning Rate: 0.001000\n      train_loss: 2.037975462608867  val_loss: 1.9958598805798424  Learning Rate: 0.000900\n      train_loss: 1.950287614762783  val_loss: 1.9718318939208985  Learning Rate: 0.000810\n      train_loss: 1.8844356292651758  val_loss: 1.9157340741819806  Learning Rate: 0.000729\n      train_loss: 1.8408661145302985  val_loss: 2.0074347290727825  Learning Rate: 0.000656\n      train_loss: 1.8106925804581908  val_loss: 1.899376357263989  Learning Rate: 0.000590\n      train_loss: 1.7870721149775717  val_loss: 1.8960167189439139  Learning Rate: 0.000531\n      train_loss: 1.7661518324580459  val_loss: 1.9048923747407065  Learning Rate: 0.000478\n      train_loss: 1.721812855783436  val_loss: 1.9142819063531027  Learning Rate: 0.000430\n      train_loss: 1.6881269902818732  val_loss: 1.9412181890673108  Learning Rate: 0.000387\n      train_loss: 1.637750076254209  val_loss: 1.9122910492950016  Learning Rate: 0.000349\n      train_loss: 1.5941708099510934  val_loss: 1.9827056364880669  Learning Rate: 0.000314\n      train_loss: 1.5721225884225634  val_loss: 1.9442243893941245  Learning Rate: 0.000282\n      train_loss: 1.5141641745550765  val_loss: 1.9996293859349357  Learning Rate: 0.000254\n      train_loss: 1.4955635654429595  val_loss: 1.9774836974011527  Learning Rate: 0.000229\n   Fold 2/5\n      train_loss: 2.388788763847616  val_loss: 2.1314850138293373  Learning Rate: 0.001000\n      train_loss: 2.0317253888481193  val_loss: 2.103186442785793  Learning Rate: 0.000900\n      train_loss: 1.962151414818234  val_loss: 2.045323813292715  Learning Rate: 0.000810\n      train_loss: 1.9066996566123433  val_loss: 2.0829850018024443  Learning Rate: 0.000729\n      train_loss: 1.8697103335625596  val_loss: 1.9868444797065523  Learning Rate: 0.000656\n      train_loss: 1.8422547712922097  val_loss: 1.9468904743591944  Learning Rate: 0.000590\n      train_loss: 1.8117327090766695  val_loss: 1.9278655072053275  Learning Rate: 0.000531\n      train_loss: 1.7707976060609023  val_loss: 1.9340546674198575  Learning Rate: 0.000478\n      train_loss: 1.738225234631035  val_loss: 1.950640723771519  Learning Rate: 0.000430\n      train_loss: 1.712116444359223  val_loss: 1.9142009152306452  Learning Rate: 0.000387\n      train_loss: 1.6669313278463151  val_loss: 1.9466180394093195  Learning Rate: 0.000349\n      train_loss: 1.6194001510739326  val_loss: 1.9448848264084921  Learning Rate: 0.000314\n      train_loss: 1.5836521152820853  val_loss: 1.9579943259557089  Learning Rate: 0.000282\n      train_loss: 1.551945626238982  val_loss: 1.9826157215568754  Learning Rate: 0.000254\n      train_loss: 1.502806189739042  val_loss: 1.950343703892496  Learning Rate: 0.000229\n   Fold 3/5\n      train_loss: 2.387221155067285  val_loss: 2.084100576241811  Learning Rate: 0.001000\n      train_loss: 2.046207749015755  val_loss: 1.962357892923885  Learning Rate: 0.000900\n      train_loss: 1.9534945203198326  val_loss: 1.884753100739585  Learning Rate: 0.000810\n      train_loss: 1.9005973404480352  val_loss: 1.8826896329720815  Learning Rate: 0.000729\n      train_loss: 1.872147830078999  val_loss: 1.884856797920333  Learning Rate: 0.000656\n      train_loss: 1.8439834874537255  val_loss: 1.8561552806033028  Learning Rate: 0.000590\n      train_loss: 1.8092140814496411  val_loss: 1.8648952749040393  Learning Rate: 0.000531\n      train_loss: 1.7703299558824963  val_loss: 1.9215360469288296  Learning Rate: 0.000478\n      train_loss: 1.7357696861856513  val_loss: 1.870755332046085  Learning Rate: 0.000430\n      train_loss: 1.6823882011903657  val_loss: 1.9308164315091239  Learning Rate: 0.000387\n      train_loss: 1.645719448890951  val_loss: 1.8818833145830367  Learning Rate: 0.000349\n      train_loss: 1.604410195433431  val_loss: 1.871110788649983  Learning Rate: 0.000314\n      train_loss: 1.554843472937743  val_loss: 1.951256067223019  Learning Rate: 0.000282\n      train_loss: 1.5110063918762737  val_loss: 1.9364427414205339  Learning Rate: 0.000254\n      train_loss: 1.473415747533242  val_loss: 1.930494589938058  Learning Rate: 0.000229\n   Fold 4/5\n      train_loss: 2.456966310739517  val_loss: 2.0371096425586277  Learning Rate: 0.001000\n      train_loss: 2.0341947904063598  val_loss: 1.9851437790526285  Learning Rate: 0.000900\n      train_loss: 1.9610237625737985  val_loss: 1.9277845591306686  Learning Rate: 0.000810\n      train_loss: 1.902164602941937  val_loss: 1.940650713443756  Learning Rate: 0.000729\n      train_loss: 1.860598719947868  val_loss: 1.937189871072769  Learning Rate: 0.000656\n      train_loss: 1.8140475385718875  val_loss: 1.9202182981703015  Learning Rate: 0.000590\n      train_loss: 1.7896782851881452  val_loss: 1.882058056526714  Learning Rate: 0.000531\n      train_loss: 1.757659265398979  val_loss: 1.91360545721319  Learning Rate: 0.000478\n      train_loss: 1.7082106782330406  val_loss: 1.9268657485644023  Learning Rate: 0.000430\n      train_loss: 1.674782997949256  val_loss: 1.9798499157031377  Learning Rate: 0.000387\n      train_loss: 1.6240646166106065  val_loss: 1.918245055940416  Learning Rate: 0.000349\n      train_loss: 1.590120876994398  val_loss: 1.947037829955419  Learning Rate: 0.000314\n      train_loss: 1.5386982994774978  val_loss: 1.9447583768102858  Learning Rate: 0.000282\n      train_loss: 1.4936983980652359  val_loss: 2.008560964796278  Learning Rate: 0.000254\n      train_loss: 1.4591720413416625  val_loss: 2.0368298209375806  Learning Rate: 0.000229\n   Fold 5/5\n      train_loss: 2.4342626286877524  val_loss: 1.9675231883923212  Learning Rate: 0.001000\n      train_loss: 2.0502510711550714  val_loss: 1.8888041151894464  Learning Rate: 0.000900\n      train_loss: 1.9545250495274862  val_loss: 1.8332275258170234  Learning Rate: 0.000810\n      train_loss: 1.9085484460824065  val_loss: 1.8527226358652116  Learning Rate: 0.000729\n      train_loss: 1.881779128478633  val_loss: 1.8398015277253257  Learning Rate: 0.000656\n      train_loss: 1.8504158890081777  val_loss: 1.8277367773983213  Learning Rate: 0.000590\n      train_loss: 1.822886474761698  val_loss: 1.81167853905095  Learning Rate: 0.000531\n      train_loss: 1.7920797733796967  val_loss: 1.8751331163777245  Learning Rate: 0.000478\n      train_loss: 1.7400302820735507  val_loss: 1.8211691856384278  Learning Rate: 0.000430\n      train_loss: 1.7206958298054007  val_loss: 1.8623224755128225  Learning Rate: 0.000387\n      train_loss: 1.6763522684574128  val_loss: 1.842137168844541  Learning Rate: 0.000349\n      train_loss: 1.6442439256442918  val_loss: 1.8643319308757782  Learning Rate: 0.000314\n      train_loss: 1.604416659474373  val_loss: 1.871770716044638  Learning Rate: 0.000282\n      train_loss: 1.5574677203264502  val_loss: 1.9138626529110803  Learning Rate: 0.000254\n      train_loss: 1.526903288728661  val_loss: 1.9068830523225997  Learning Rate: 0.000229\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"submission = pd.DataFrame(data={'ID': test_data['ID'], 'prediction': test_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:37:54.385963Z","iopub.execute_input":"2025-03-03T08:37:54.386567Z","iopub.status.idle":"2025-03-03T08:37:54.400334Z","shell.execute_reply.started":"2025-03-03T08:37:54.386534Z","shell.execute_reply":"2025-03-03T08:37:54.399225Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:38:06.286890Z","iopub.execute_input":"2025-03-03T08:38:06.287290Z","iopub.status.idle":"2025-03-03T08:38:06.310218Z","shell.execute_reply.started":"2025-03-03T08:38:06.287258Z","shell.execute_reply":"2025-03-03T08:38:06.309147Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      ID  prediction\n0  28800   -1.133949\n1  28801    0.415221\n2  28802   -1.294203","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28800</td>\n      <td>-1.133949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28801</td>\n      <td>0.415221</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28802</td>\n      <td>-1.294203</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}